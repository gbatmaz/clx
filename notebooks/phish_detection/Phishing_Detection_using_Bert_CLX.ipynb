{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Detection Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Eli Fajardo (NVIDIA)\n",
    "- Gorkem Batmaz (NVIDIA)\n",
    "- Bartley Richardson, PhD (NVIDIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "* Introduction\n",
    "* List of datasets used\n",
    "* Reading in the datasets\n",
    "* Initialize CLX Phishing Detection and BERT model\n",
    "* Training - CLAIR FRAUDULENT EMAILS dataset\n",
    "* Evaluation of CLAIR Test Set\n",
    "* Training with the the SPAM_ASSASSIN dataset\n",
    "* Evaluation of the SPAM_ASSASSIN Test Set\n",
    "* Training with all three datasets CLAIR+SPAM_ASSASSIN+ENRON\n",
    "* Evaluation of the Test Set of CLAIR+SPAM_ASSASSIN+ENRON Datasets\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Phishing is a method used by fraudsters/hackers to obtain sensitive information from email users by pretending to be from legitimate institutions/people.\n",
    "Various machine learning methods are in use to detect and filter phishing/spam emails. \n",
    "In this notebook, we show how to train a *BERT language model and analyse the performance on various datasets. We have fine-tuned a pre-trained BERT model with a classification layer using HuggingFace library. \n",
    "*BERT stands for Bidirectional Encoder Representations from Transformers. The paper can be found [here.](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "This notebook will be updated with a much faster GPU tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets used\n",
    "* [CLAIR-Fraudulent E-mail Corpus](https://www.kaggle.com/rtatman/fraudulent-email-corpus)\n",
    "* [SPAM_ASSASSIN Dataset](https://spamassassin.apache.org/old/publiccorpus/)\n",
    "* [Enron Emails](https://www.cs.cmu.edu/~enron/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from clx.analytics.phishing_detector import PhishingDetector\n",
    "import s3fs\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAIR_TSV = \"Phishing_Dataset_Clair_Collection.tsv\"\n",
    "SPAM_TSV = \"spam_assassin_spam_200_20021010.tsv\"\n",
    "EASY_HAM_TSV = \"spam_assassin_easyham_200_20021010.tsv\"\n",
    "HARD_HAM_TSV = \"spam_assassin_hardham_200_20021010.tsv\"\n",
    "ENRON_TSV = \"enron_10000.tsv\"\n",
    "\n",
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clair dataset\n",
    "if not path.exists(CLAIR_TSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + CLAIR_TSV, CLAIR_TSV)\n",
    "    \n",
    "dfclair = cudf.read_csv(CLAIR_TSV, delimiter='\\t', header=None, names=['label', 'email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phishing emails of the SPAM ASSASSIN dataset\n",
    "if not path.exists(SPAM_TSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + SPAM_TSV, SPAM_TSV)\n",
    " \n",
    "dfspam = cudf.read_csv(SPAM_TSV, delimiter='\\t', header=None, names=['label', 'email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benign emails of the SPAM ASSASSIN dataset\n",
    "if not path.exists(EASY_HAM_TSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + EASY_HAM_TSV, EASY_HAM_TSV)\n",
    "    \n",
    "dfeasyham = cudf.read_csv(EASY_HAM_TSV, delimiter='\\t', header=None, names=['label', 'email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benign emails of the SPAM ASSASSIN dataset that are easy to be confused with phishing emails\n",
    "if not path.exists(HARD_HAM_TSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + HARD_HAM_TSV, HARD_HAM_TSV)\n",
    "\n",
    "dfhardham = cudf.read_csv(HARD_HAM_TSV, delimiter='\\t', header=None, names=['label', 'email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benign Enron emails\n",
    "if not path.exists(ENRON_TSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + ENRON_TSV, ENRON_TSV)\n",
    "\n",
    "dfenron = cudf.read_csv(ENRON_TSV, delimiter='\\t', header=None, names=['label', 'email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The files contain the first 200 words of each email. The model uses only the first 128 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize/Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_detect = PhishingDetector()\n",
    "phish_detect.init_model()\n",
    "\n",
    "# init_model can also load pre-trained model by passing it model directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - CLAIR FRAUDULENT EMAILS DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfclair, 'label', train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_detect.train_model(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of CLAIR Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with SPAM_ASSASSIN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the spam assasin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assassin = cudf.concat([dfhardham,dfeasyham,dfspam], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_assassin, 'label', train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_detect.train_model(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the SPAM_ASSASSIN Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_detect.evaluate_model(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with CLAIR+SPAM_ASSASSIN datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two datasets and split as train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = cudf.concat([dfhardham,dfeasyham,dfspam,dfclair],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_total, 'label', train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_detect.train_model(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Test Set of CLAIR+SPAM_ASSASSIN Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_detect.evaluate_model(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with all three datasets (CLAIR+SPAM_ASSASSIN+ENRON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all the datasets, split into training and test set and then tokenize the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = cudf.concat([dfhardham,dfeasyham,dfspam,dfclair,dfenron],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_total, 'label', train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_detect.train_model(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Test Set of CLAIR+SPAM_ASSASSIN+ENRON Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_detect.evaluate_model(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* https://github.com/huggingface/transformers/tree/master/examples#\n",
    "* https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n",
    "* https://github.com/ThilinaRajapakse/pytorch-transformers-classification\n",
    "* https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of BERT Fine-Tuning Sentence Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
